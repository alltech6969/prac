 <pre class="ndfHFb-c4YZDc-fmcmS-DARUcf" style="-webkit-tap-highlight-color: transparent; background-color: white; font-family: &quot;Courier New&quot;, Courier, monospace, arial, sans-serif; font-size: 16px; font-weight: 600; margin-bottom: 0px; margin-top: 0px; overflow-wrap: break-word; text-align: start; text-wrap: wrap; user-select: text;">CC

Shreyash ka link : <a href="https://notepad.pw/dsprac2">https://notepad.pw/dsprac2</a>
	 
Yash ka link : <a href="https://notepad.pw/dsprac1">https://notepad.pw/dsprac1</a>
	 
Practical 1 (MS Excel):

1.Conditional Formatting
	1.Highlight Cells
	2.Top/Bottom Rules (Average->Below/Above)
	3.Data bars
	4.Color sets
	5.Icon sets
	6.Duplicate values (New rule -> 2nd last option(only unique or...)->strikethrough)
2.Pivot the file
	1.Drag and drop product in row labels -> amount in values -> dropdown option -> value field settings -> count
	2.Drag and drop category in column labels
	3.Drag and drop country in report labels
	4.Sort => select cell -> right click -> sort -> smallest to largest or largest to smallest
	5.Pivot table chart 2d column:
		Amount-> Value
		Product-> Axis
		Country-> Filter
		Category-> Legend

3.VLookUP
	1. within a sheet:
		=vlookup(A5,A1:B6,2,)
	2. betwwen two sheets:
		=vlookup(A4,Sheet2!A2:B4,2,)
		
4.What-if analysis using gaol seek
	1.select the cell to be changed
	2.enter the target value
	3.select cels which has to be changed

Practical 2 (R) :

A.Reading file

data = read.csv(file.choose())
print(head(data())

library(rjson)
data=fromJSON(file=file.choose())
print(data)

B.Basic data pre-processing

data = read.csv(file.choose())
dim(data)
variable_names=names(data)
print(variable_names)
str(data)

colSums(is.na(data))
column=c("Pclass")
frequency = table(data&Pclass)
print(frequency)
node=names(frequency)[which.max(frequency)]
print(node)
data$Pclass[is.na(data%Pclass)]=3
print(head(data))

C.Manipulate and transform (filtering,sorting and grouping)

install.packages("dplyr")
library(dplyr)
stats=data.frame(player=c('A','B','C','D'),runs=c(100,200,408,19),wickets=c(1,7,20,NA,5))
print(stats)
filter(stats,runs>100)

stats=data.frame(player=c('A','B','C','D','A','A'),runs=c(100,200,408,19,56,100),wickets=c(1,7,20,NA,52,17))
distinct(stats)
unique(stats)
distinct(stats,player,.keep_all=TRUE)

stats=data.frame(player=c('A','B','C','D'),runs=c(100,200,408,19),wickets=c(1,7,20,NA,5))
arrange(stats,runs)

data_frame=data.frame(col1=sample(6:7,9,replace=TRUE),col2=letters[1:3],col3=c(1,4,5,1,NA,NA,2,NA,2))
print(data_frame)

data_frame%>%group_by(col1)


Practical 2 (Python) :

A. Read data from CSV and JSON files into a data frame:

# Read data from a csv file
import pandas as pd
df = pd.read_csv('Student_Marks.csv')
print("Our dataset ")
print(df)

# Reading data from a JSON file
import pandas as pd
data = pd.read_json('dataset.json')
print(data)

B. Perform basic data pre-processing tasks such as handling missing values and outliers:

# Replacing NA values using fillna()
import pandas as pd

df = pd.read_csv('titanic.csv')
print(df)
df.head(10)
print("Dataset after filling NA values with 0 : ")
df2=df.fillna(value=0)
print(df2)

# Dropping NA values using dropna()
import pandas as pd
df = pd.read_csv('titanic.csv')
print(df)
df.head(10)
print("Dataset after dropping NA values: ")
df.dropna(inplace = True)
print(df)

C. Manipulate and transform data using functions like filtering, sorting, and grouping: 

import pandas as pd
# Load iris dataset
iris = pd.read_csv('Iris.csv')
# Filtering data based on a condition
setosa = iris[iris['Species'] == 'setosa']
print("Setosa samples:")
print(setosa.head())
# Sorting data
sorted_iris = iris.sort_values(by='SepalLengthCm', ascending=False)
print("\nSorted iris dataset:")

print(sorted_iris.head())
# Grouping data
grouped_species = iris.groupby('Species').mean()
print("\nMean measurements for each species:")
print(grouped_species)


Practical 3 (R) :

A.Feature-scaling techniques like standardiztion and normaliztion

install.packages("dplyr")
install.packages("caret")
library(dplyr)
library(caret)

#housing.csv
mydata=read.csv(file.choos())
print(mydata)

#normalization using log
processed_data=mydata
processed_data=processed_data %>% mutate(price_log =log(processed_data$Price))
print(processed_data)

#normalization using scale
processed_data=processed_data %>% mutate(price_scale =scale(processed_data$Price))
print(processed_data)

#normalization using z-score
processed_data=mydata
price_mean=mean(processed_data$Price)
price_sd=sd(processed_data$Price)
processed_data=processed_data%>%mutate(Price_zscore=(processed_data$Price-price_mean)/price_sd)
print(processed_data)

#standardiztion
df = pre_process(processed_data, method=c("range"))
processed_data = predict(df,as.data.frame(processed_data))
print(processed_data)

B. Feature dummification (categorical to numerical)

str(PlantGrowth)
head(PlantGrowth)
tail(PlantGrowth)
pg=PlantGrowth
head(pg,20)
pg$group_ctrl=ifelse(pg$group=='ctrl',1,0)
head(pg,20)

#using dummy cols
install.packages("fastDummies")
library(fastDummies)

data=PlantGrowth
data=dummy_cols(data,select_columns="group")
print(data)


Practical 3 (Python) :

A. Apply feature-scaling techniques like standardization and normalization to numerical
features:

# Standardization and normalization
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
df = pd.read_csv('wine.csv', header=None, usecols=[0, 1, 2], skiprows=1)
df.columns = ['classlabel', 'Alcohol', 'Malic Acid']
print("Original DataFrame:")
print(df)
scaling=MinMaxScaler()
scaled_value=scaling.fit_transform(df[['Alcohol','Malic Acid']])
df[['Alcohol','Malic Acid']]=scaled_value
print("\n Dataframe after MinMax Scaling")
print(df)
scaling=StandardScaler()
scaled_standardvalue=scaling.fit_transform(df[['Alcohol','Malic Acid']])
df[['Alcohol','Malic Acid']]=scaled_standardvalue
print("\n Dataframe after Standard Scaling")
print(df)

B. Perform feature Dummification to convert categorical variables into numerical representations:

import pandas as pd
iris=pd.read_csv("Iris.csv")
print(iris)
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
iris['code']=le.fit_transform(iris.Species)
print(iris)


Practical 4:

A. One sample T-test

import numpy as np
from scipy import stats
marks = [90, 100, 80, 95, 92, 45]
print("Marks:", marks)
mean = np.mean(marks)
print("Mean:", mean)
t_test, p_val = stats.ttest_1samp(marks, 80)
alpha = 0.05
print("Null Hypothesis: The sample mean of marks is equal to 80.")
print("Alternative Hypothesis: The sample mean of marks is not equal to 80.")
if p_val < alpha:
	print("Result:  Reject  the  null  hypothesis.  There  is  a significant   difference   between   the   sample   mean   and   the population mean (80).")
else:
	print("Result:  Accept  the  null  hypothesis.  There  is  no significant   difference   between   the   sample   mean   and   the population mean (80).")

B. Two sample T-test

import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

np.random.seed(42)
sample1 = np.random.normal(loc=10, scale=2, size=30)
sample2 = np.random.normal(loc=12, scale=2, size=30)

t_statistic, p_value = stats.ttest_ind(sample1, sample2)

alpha = 0.05
print("Results of Two-Sample t-test:")
print(f'T-statistic: {t_statistic}')
print(f'P-value: {p_value}')
print(f"Degrees of Freedom: {len(sample1) + len(sample2) - 2}")

#for plotting
plt.figure(figsize=(10, 6))
plt.hist(sample1, alpha=0.5, label='Sample 1', color='blue')
plt.hist(sample2, alpha=0.5, label='Sample 2', color='orange')
plt.axvline(np.mean(sample1), color='blue', linestyle='dashed', linewidth=2)
plt.axvline(np.mean(sample2), color='orange', linestyle='dashed', linewidth=2)
plt.title('Distributions of Sample 1 and Sample 2')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.legend()

if p_value < alpha:
    critical_region = np.linspace(min(sample1.min(), sample2.min()), max(sample1.max(),sample2.max()), 1000)
    plt.fill_between(critical_region, 0, 5, color='red', alpha=0.3, label='Critical Region')
    plt.text(11, 5, f'T-statistic: {t_statistic:.2f}', ha='center', va='center', color='black',backgroundcolor='white')

plt.show()

if p_value < alpha:
    if np.mean(sample1) > np.mean(sample2):
        print("Conclusion: There is significant evidence to reject the null hypothesis.")
        print("Interpretation: The mean of Sample 1 is significantly higher than that of Sample2.")
    else:
        print("Conclusion: There is significant evidence to reject the null hypothesis.")
        print("Interpretation: The mean of Sample 2 is significantly higher than that of Sample1.")
else:
    print("Conclusion: Fail to reject the null hypothesis.")
    print("Interpretation: There is not enough evidence to claim a significant difference between the means.")

C. chi-square test

import pandas as pd
import numpy as np
import matplotlib as plt
import seaborn as sb
import warnings
from scipy import stats
warnings.filterwarnings('ignore')
df=sb.load_dataset('mpg')
print(df)
print(df['horsepower'].describe())
print(df['model_year'].describe())
bins=[0,75,150,240]
df['horsepower_new']=pd.cut(df['horsepower'],bins=bins,labels=['l','m','h'])
c=df['horsepower_new']
print(c)
ybins=[69,72,74,84]
label=['t1','t2','t3']
df['modelyear_new']=pd.cut(df['model_year'],bins=ybins,labels=label)
newyear=df['modelyear_new']
print(newyear)
df_chi=pd.crosstab(df['horsepower_new'],df['modelyear_new'])
print(df_chi)
stat,p_val,dof,expected=stats.chi2_contingency(df_chi)
print("stat: ",stat)
print("p_val: ",p_val)
print("dof: ",dof)
print("expected: ",expected)


Practical 5 : Analysis of variance (ANOVA)

import pandas as pd
import scipy.stats as stats
import numpy as np
from statsmodels.stats.multicomp import pairwise_tukeyhsd
group1 = [23, 25, 29, 34, 30]
group2 = [19, 20, 22, 24, 25]
group3 = [15, 18, 20, 21, 17]
group4 = [28, 24, 26, 30, 29]

m1=np.mean(group1)
m2=np.mean(group2)
m3=np.mean(group3)
m4=np.mean(group4)
print("Means: ",m1,m2,m3,m4)

f_statistics, p_value = stats.f_oneway(group1, group2, group3, group4)
print("one-way ANOVA:")
print("F-statistics:", f_statistics)
print("p-value", p_value)

all_data = group1 + group2 + group3 + group4
group_labels = ['Group1'] * len(group1) + ['Group2'] * len(group2) + ['Group3'] *len(group3) + ['Group4'] * len(group4)

tukey_results = pairwise_tukeyhsd(all_data, group_labels)
print("\nTukey-Kramer post-hoc test:")
print(tukey_results)

alpha=0.05
if p_value<=alpha:
    print("Reject null hypothesis")
else:
    print("Accept null hypothesis")


Practical 6:

A1. Linear Regression (Python)

import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
housing = fetch_california_housing()
housing_df = pd.DataFrame(housing.data, columns=housing.feature_names)
print(housing_df)
housing_df['PRICE'] = housing.target
X = housing_df[['AveRooms']]
y = housing_df['PRICE']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
mse = mean_squared_error(y_test, model.predict(X_test))
r2 = r2_score(y_test, model.predict(X_test))

print("Mean Squared Error:", mse)
print("R-squared:", r2)
print("Intercept:", model.intercept_)
print("Coefficient:", model.coef_)'

A2. Linear regression (R)

X=c(151,150,162,138,162,179)
Y=c(60,61,62,55,62,70)
r=lm(Y~X)
print(r)
plot(r)
h=data.frame(X=180)
result=predict(r,h)
print(result)

B. Multiple Linear Regression)

X = housing_df.drop('PRICE',axis=1)
y = housing_df['PRICE']
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
model = LinearRegression()
model.fit(X_train,y_train)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test,y_pred)
r2 = r2_score(y_test,y_pred)
print("Mean Squared Error:",mse)
print("R-squared:",r2)
print("Intercept:",model.intercept_)
print("Coefficient:",model.coef_)


Practical 7 :

A.Logiostic Regression Model

install.packages("dplyr")
install.packages("caTools")
library(dplyr)
library(caTools)

head(mtcars)
summary(mtcars)
split<-sampl.spli(mtcars,SplitRatio=0.8)
split

train_reg = subset(mtcars,split=="TRUE")
test_reg=(mtcars,split=="FALSE")
logistic_model=glm(vs~wt+disp,data=train_reg,family="binomial")
logistic_model
summary(logistic_model)

B. Performance evaluation

install.packages("ROCR")
library(ROCR)

predict_reg = predict(logistic_model,test_reg,type="response")
predict_reg

predict_reg=ifelse(predict_reg>0.5,1,0)
table(test_reg$vs,predict_reg)

missing_classerr= mean(predict_reg != test_reg$vs)
print(paste('Accuracy=',1-missing_classerr))

ROCPred=prediction(predict_reg,test_reg$vs)
ROCPer=performance(ROCPred,measure="tpr",x.measure="fpr")
auc=performance(ROCPred,measure="auc")
auc=auc@y.values[[1]]
auc

plot(ROCPer)
plot(ROCPer,colorize=TRUE,print.cutoffs.at=seq(0.1,by=0.1),main="ROC CURVE")
abline(a=0,b=1)
auc=round(auc,4)
legend(.6,.4,auc,title="AUC",cex=1)

C. Decision Tree

install.packages(c("rpart","rpart.plot"))
library(rpart)
library(rpart.plot)
data(iris)
tree=rpart(Species~.,data=iris,method="class")
rpart.plot(tree,extra=106)


Practical 7 (Python) :Logistic regression and Decision Tree

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report

iris = load_iris()
iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] +
['target'])
binary_df = iris_df[iris_df['target'] != 2]
X = binary_df.drop('target', axis=1)
y = binary_df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)
y_pred_logistic = logistic_model.predict(X_test)
print("Logistic Regression Metrics")
print("Accuracy: ", accuracy_score(y_test, y_pred_logistic))
print("Precision:", precision_score(y_test, y_pred_logistic))
print("Recall: ", recall_score(y_test, y_pred_logistic))

print("\nClassification Report")
print(classification_report(y_test, y_pred_logistic))

decision_tree_model = DecisionTreeClassifier()
decision_tree_model.fit(X_train, y_train)
y_pred_tree = decision_tree_model.predict(X_test)
print("\nDecision Tree Metrics")
print("Accuracy: ", accuracy_score(y_test, y_pred_tree))
print("Precision:", precision_score(y_test, y_pred_tree))
print("Recall: ", recall_score(y_test, y_pred_tree))
print("\nClassification Report")
print(classification_report(y_test, y_pred_tree))


Practical 8 : k-means clustering

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
data = pd.read_csv("wholesale.csv")
data.head()
categorical_features = ['Channel', 'Region']
continuous_features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']
data[continuous_features].describe()
for col in categorical_features:
dummies = pd.get_dummies(data[col], prefix = col)
data = pd.concat([data, dummies], axis = 1)
data.drop(col, axis = 1, inplace = True)
data.head()
mms = MinMaxScaler()
mms.fit(data)
data_transformed = mms.transform(data)
sum_of_squared_distances = []
K = range(1, 15)
for k in K:
km = KMeans(n_clusters=k)
km = km.fit(data_transformed)

sum_of_squared_distances.append(km.inertia_)
plt.plot(K, sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('sum_of_squared_distances')
plt.title('elbow Mehtod for optimal k')
plt.show()


Practical 9: Principle Component Analysis (PCA)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
iris = load_iris()
iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] +
['target'])
X = iris_df.drop('target', axis=1)
y = iris_df['target']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
pca = PCA()
X_pca = pca.fit_transform(X_scaled)
explained_variance_ratio = pca.explained_variance_ratio_
plt.figure(figsize=(8, 6))
plt.plot(np.cumsum(explained_variance_ratio), marker='o', linestyle='--')
plt.title('Explained Variance Ratio')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.grid(True)

plt.show()
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)
n_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1
print(f"Number of principal components to explain 95% variance: {n_components}")
pca = PCA(n_components=n_components)
X_reduced = pca.fit_transform(X_scaled)
plt.figure(figsize=(8, 6))
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap='viridis', s=50, alpha=0.5)
plt.title('Data in Reduced-dimensional Space')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Target')
plt.show()


Practical 10: Data visualization and story telling

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
# Generate random data
np.random.seed(42)  # Set a seed for reproducibility
# Create a DataFrame with random data
data = pd.DataFrame({
'variable1': np.random.normal(0, 1, 1000),
'variable2': np.random.normal(2, 2, 1000) + 0.5 * np.random.normal(0, 1, 1000),
'variable3': np.random.normal(-1, 1.5, 1000),
'category': pd.Series(np.random.choice(['A', 'B', 'C', 'D'], size=1000, p=[0.4, 0.3, 0.2, 0.1]),
dtype='category')
})
# Create a scatter plot to visualize the relationship between two variables
plt.figure(figsize=(10, 6))
plt.scatter(data['variable1'], data['variable2'], alpha=0.5)
plt.title('Relationship between Variable 1 and Variable 2', fontsize=16)
plt.xlabel('Variable 1', fontsize=14)
plt.ylabel('Variable 2', fontsize=14)
plt.show()

# Create a bar chart to visualize the distribution of a categorical variable
plt.figure(figsize=(10, 6))
sns.countplot(x='category', data=data)
plt.title('Distribution of Categories', fontsize=16)
plt.xlabel('Category', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(rotation=45)
plt.show()
# Create a heatmap to visualize the correlation between numerical variables
plt.figure(figsize=(10, 8))
numerical_cols = ['variable1', 'variable2', 'variable3']
sns.heatmap(data[numerical_cols].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap', fontsize=16)
plt.show()
# Data Storytelling
print("Title: Exploring the Relationship between Variable 1 and Variable 2")
print("\nThe scatter plot (Figure 1) shows the relationship between Variable 1 and Variable 2.
We can observe a positive correlation, indicating that as Variable 1 increases, Variable 2 tends to
increase as well. However, there is a considerable amount of scatter, suggesting that other factors
may influence this relationship.")
print("\nScatter Plot")
print("Figure 1: Scatter Plot of Variable 1 and Variable 2")
print("\nTo better understand the distribution of the categorical variable 'category', we created a
bar chart (Figure 2). The chart reveals that Category A has the highest frequency, followed by
Category B, Category C, and Category D. This information could be useful for further analysis
or decision-making processes.")

print("\nBar Chart")
print("Figure 2: Distribution of Categories")
print("\nAdditionally, we explored the correlation between numerical variables using a heatmap
(Figure 3). The heatmap shows that Variable 1 and Variable 2 have a strong positive correlation,
confirming the observation from the scatter plot. However, we can also see that Variable 3 has a
moderate negative correlation with both Variable 1 and Variable 2, suggesting that it may have
an opposing effect on the relationship between the first two variables.")
print("\nHeatmap")
print("Figure 3: Correlation Heatmap")
print("\nIn summary, the visualizations and analysis provide insights into the relationships
between variables, the distribution of categories, and the correlations between numerical
variables. These findings can be used to inform further analysis, decision-making, or to generate
new hypotheses for investigation.")


============================================================================================================================================================================================Question's solution

KMeans clustering ka he

mport numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Load Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Silhouette analysis for determining optimal number of clusters
silhouette_scores = []
for n_clusters in range(2, 11):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)
    silhouette_avg = silhouette_score(X_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Visualize silhouette scores
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis for KMeans Clustering')
plt.show()

# Choose optimal number of clusters based on silhouette score
optimal_n_clusters = np.argmax(silhouette_scores) + 2
print("Optimal number of clusters:", optimal_n_clusters)

# Perform KMeans clustering with optimal number of clusters
kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42)
cluster_labels = kmeans.fit_predict(X_scaled)

# Visualize clustering results (using PCA for visualization)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8, 6))
for i in range(optimal_n_clusters):
    plt.scatter(X_pca[cluster_labels == i, 0], X_pca[cluster_labels == i, 1], label=f'Cluster {i+1}')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroids')
plt.title('KMeans Clustering of Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

t-test

import numpy as np
from scipy import stats

np.random.seed(42)

sample1 = np.array([85, 95, 100, 80, 90, 97, 104, 95, 88, 92, 94, 99])
sample2 = np.array([83, 85, 96, 92, 100, 104, 94, 95, 88, 90, 93, 94])

t_statistic, p_value = stats.ttest_ind(sample1, sample2)

alpha = 0.05
print("Results of Two-Sample t-test:")
print(f"t-statistic: {t_statistic}")
print(f"p-value: {p_value}")
print(f"Degrees of Freedom: {len(sample1) + len(sample2) - 2}")

if p_value < alpha:
    if np.mean(sample1) > np.mean(sample2):
        print("Conclusion: There is significant evidence to reject the null hypothesis.")
        print("Interpretation: The mean caffeine content of Sample 1 is significantly higher than that of Sample 2.")
    else:
        print("Conclusion: There is significant evidence to reject the null hypothesis.")
        print("Interpretation: The mean caffeine content of Sample 2 is significantly higher than that of Sample 1.")
else:
    print("Conclusion: Fail to reject the null hypothesis.")
    print("Interpretation: There is not enough evidence to claim a significant difference between the means.")

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

linear Regression load irris

# Load the Iris dataset
data(iris)
df <- iris

# Fit linear regression model
model <- lm(petal.width ~ petal.length, data = df)

# Print the summary of the model
summary(model)

# Load the required library
library(ggplot2)

# Create a scatter plot of petal.width against petal.length
ggplot(df, aes(x = petal.length, y = petal.width)) +
  geom_point() +  # Add data points
  geom_smooth(method = "lm", se = FALSE, color = "blue") +  # Add linear regression line
  labs(x = "Petal Length", y = "Petal Width", title = "Linear Regression on Iris Dataset")  # Add axis labels and title

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

logistic regression load iris

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import load_iris

# Load Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Consider only two classes: setosa (class 0) and non-setosa (class 1)
X_binary = X[y != 0]
y_binary = y[y != 0]

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_binary, y_binary, test_size=0.2, random_state=42)

# Fit logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:")
print(report)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

multiple linear regression diabetes data

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_diabetes

# Load Pima Indian Diabetes dataset
diabetes = load_diabetes()
X = diabetes.data
y = diabetes.target

# Convert to DataFrame for better understanding
df = pd.DataFrame(data=X, columns=diabetes.feature_names)
df['Target'] = y

# Features and target variable
X = df.drop('Target', axis=1)
y = df['Target']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit Multiple Linear Regression model
mlr_model = LinearRegression()
mlr_model.fit(X_train, y_train)

# Predictions
y_pred = mlr_model.predict(X_test)

# Evaluate model performance
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

FOR 3 SAMPLE WITH DIAGRAM
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import f_oneway

# Define the class grades
class_A = [85, 90, 88, 82, 87]
class_B = [76, 78, 80, 81, 75]
class_C = [92, 88, 94, 89, 90]

# Calculate the mean grades for each class
mean_A = np.mean(class_A)
mean_B = np.mean(class_B)
mean_C = np.mean(class_C)

# Perform one-way ANOVA
f_statistic, p_value = f_oneway(class_A, class_B, class_C)

# Print the results
print("F-statistic:", f_statistic)
print("P-value:", p_value)

if p_value < 0.05:
    print("Reject null hypothesis: There is a significant difference between the means of the classes.")
else:
    print("Fail to reject null hypothesis: There is no significant difference between the means of the classes.")

# Create bar graph
classes = ['Class A', 'Class B', 'Class C']
means = [mean_A, mean_B, mean_C]

plt.bar(classes, means, color=['blue', 'green', 'red'])
plt.title('Mean Grades of Each Class')
plt.ylabel('Mean Grade')
plt.xlabel('Classes')

# Show plot
plt.show()
</pre>
